# -*- coding: utf-8 -*-
"""ID2214.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A0N1bpnSOWVt31L225zXOie1Ej2vHnIR

# ID2214 Programming for Data Science

### Assignment 4

Authors: \
Lucas Larsson lulars@kth.se \
Mihaela Bakšić baksic@kth.se
"""

! pip install rdkit
! pip install molmass

# Defining input and output file paths
TRAIN_DATASET_PATH = '/content/gdrive/My Drive/ID2214/training_smiles.csv'
TEST_DATASET_PATH = '/content/gdrive/My Drive/ID2214/test_smiles.csv'
FEATURES_OPTIM_RES_PATH = '/content/gdrive/My Drive/ID2214/features_optim.csv'
FINGERPRINT_OPTIM_RES_PATH = '/content/gdrive/My Drive/ID2214/fingerprint_size_opt.csv'
TRAIN_DATASET_PREPARED_PATH = '/content/gdrive/My Drive/ID2214/training_prepared.csv'
TEST_DATASET_PREPARED_PATH = '/content/gdrive/My Drive/ID2214/test_prepared.csv'

CUSTOM_RES_PATH = '/content/gdrive/My Drive/ID2214/custom_res.csv'
FP_RES_PATH = '/content/gdrive/My Drive/ID2214/fp_res.csv'
COMBINED_RES_PATH =  '/content/gdrive/My Drive/ID2214/combined_res.csv'

RESULT_PATH = '/content/gdrive/My Drive/ID2214/4.csv'

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
from rdkit import Chem
from rdkit.Chem import ChemicalFeatures, AllChem, Draw, rdmolops, Lipinski, rdMolDescriptors, Fragments,Crippen
from rdkit import RDConfig
from molmass import Formula
import os
import re

"""## Data import and feature extraction"""

train_data = pd.read_csv(TRAIN_DATASET_PATH, dtype = {'ACTIVE': int})

train_data.head()

train_data[['INDEX', 'ACTIVE']].groupby('ACTIVE').count()

"""### Feature extracting functions"""

def extract_charge(df):
  '''
    Extracts molecule charge from the mol column of the dataframe
    Returns charge series
  '''
  return df['MOL'].apply(lambda x: rdmolops.GetFormalCharge(x))


def extract_mol(df):
  '''
    Extracts molecule object from the smiles column of the dataframe
    Returns molecule series
  '''
  return df['SMILES'].apply(lambda x: Chem.MolFromSmiles(x))


def extract_num_atoms(df, atom_symbol):
  '''
    Extracts number of atoms with a given symbol from the molecule
    Returns count series
  '''

  m = Chem.MolFromSmarts(atom_symbol)
  return df['MOL'].apply(lambda x: len(x.GetSubstructMatches(m)))


def extract_fingerprints(df, size_fp=124):
  '''
    Extracts fingerprint vectors from smiles columns
    Returns a dataframe where each column is a fingerprint bit and a list of fingerprint feature names
  '''

  fingerprints = df['MOL'].apply(lambda x: AllChem.GetMorganFingerprintAsBitVect(x, 2, nBits=size_fp))
  fp_features = [f'fp_{i}' for i in range(size_fp)]

  fingerprints_array = [ [ fp[i] for i in range(size_fp) ] for fp in fingerprints]
  return pd.DataFrame(data=fingerprints_array, columns=fp_features), fp_features


def extract_heavy_atom_count(df):
  '''
  Extracts number of heavy atoms from the molecule
  Returns a series with the heavy atom count
  '''
  return df['MOL'].apply(lambda x: Lipinski.HeavyAtomCount(x))

def extract_NHOH(df):

  return df['MOL'].apply(lambda x: Lipinski.NHOHCount(x))

def extract_num_rings(df):
  return df['MOL'].apply(lambda x: Lipinski.RingCount(x))
  
def extract_num_aromatic_rings(df):
  return df['MOL'].apply(lambda x: Lipinski.NumAromaticRings(x))

def extract_num_saturated_rings(df):
  return df['MOL'].apply(lambda x: Lipinski.NumSaturatedRings(x))


def extract_donors_and_acceptors(df):
  '''
  Extracts number of hydrogen Acceptors and Donors respectivlly 
  Returns two seriess containing the number bondes of 'Acceptors' and 'Donors'
  '''
  acceptors = df['MOL'].apply(lambda x: Lipinski.NumHAcceptors(x))
  donors = df['MOL'].apply(lambda x: Lipinski.NumHDonors(x))
  return acceptors, donors
  

def calculate_mass(df):
  '''
  Calculates the mass of a molecule
  Returns a series with mass of molecule in dalton
  '''
  return df['MOL'].apply(lambda x: Formula(re.sub(r"-\d+","", rdMolDescriptors.CalcMolFormula(x))).mass)


def extract_Al_OOF(df):
  return df['MOL'].apply(lambda x: Fragments.fr_Al_COO(x))

def extract_Al_OH(df):
  return df['MOL'].apply(lambda x: Fragments.fr_Al_OH(x))

def extract_saturated_carbocycles(df):
  return df['MOL'].apply(lambda x: Lipinski.NumSaturatedCarbocycles(x))

def extract_saturated_heterocycles(df):
  return df['MOL'].apply(lambda x: Lipinski.NumSaturatedHeterocycles(x))


def extract_functional_groups(df):
  return df['MOL'].apply(lambda x: Fragments.fr_ArN(x))


def extract_log_p(df):
  return df['MOL'].apply(lambda x: Crippen.MolLogP(x))


def extract_mol_mr(df):
  return df['MOL'].apply(lambda x: Crippen.MolMR(x))


def calc_rotatable_bonds(df):
  return df['MOL'].apply(lambda x: rdMolDescriptors.CalcNumRotatableBonds(x))

"""## Applying feature extraction

Features extracted from SMILEs can be categorised in 2 groups: 
- fingerprint vector features
- custom features extracted using rdkit and molmass libraries

We use this feature groups together and separately to determine the most suitable feature set, based on model performances.
"""

train_data['MOL'] = extract_mol(train_data)
train_data['CHARGE'] = extract_charge(train_data)
train_data['NUM_C'] = extract_num_atoms(train_data, 'C')
train_data['NUM_O'] = extract_num_atoms(train_data, 'O')
train_data['NUM_N'] = extract_num_atoms(train_data, 'N')
train_data['NUM_HEAVY'] = extract_heavy_atom_count(train_data)
train_data['MASS'] = calculate_mass(train_data)
train_data['ACCEPTORS'], train_data['DONORS'] = extract_donors_and_acceptors(train_data)
train_data['AL_OOF'] = extract_Al_OOF(train_data)
train_data['AL_OH'] = extract_Al_OH(train_data)
train_data['NUM_NHOH'] = extract_NHOH(train_data)
train_data['NUM_RINGS'] = extract_num_rings(train_data)
train_data['NUM_AROM_RINGS'] = extract_num_aromatic_rings(train_data)
train_data['NUM_SAT_RINGS'] = extract_num_saturated_rings(train_data)
train_data['NUM_HETCYCLES'] = extract_saturated_heterocycles(train_data)
train_data['NUM_CARBCYCLES'] = extract_saturated_carbocycles(train_data)

train_data['ROTATABLE_BONDS'] = calc_rotatable_bonds(train_data)
train_data['LOG_P'] = extract_log_p(train_data)
train_data['FUNC_GROUP'] = extract_functional_groups(train_data)
train_data['MR'] = extract_mol_mr(train_data)


# fingerprints

size_fp = 512
fingerprints, fp_features = extract_fingerprints(train_data, size_fp)
train_data = train_data.join(fingerprints)

custom_features_all = ['CHARGE','MASS', 'NUM_C', 'NUM_O', 'NUM_N', 'NUM_HEAVY', 
                   'ACCEPTORS', 'DONORS', 'AL_OOF', 'AL_OH', 'NUM_NHOH', 
                   'NUM_RINGS', 'NUM_AROM_RINGS', 'NUM_SAT_RINGS',
                   'NUM_HETCYCLES', 'NUM_CARBCYCLES',
                   'ROTATABLE_BONDS', 'LOG_P', 'FUNC_GROUP', 'MR']

train_data.head()

from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold, KFold

from imblearn.over_sampling import SMOTE, RandomOverSampler
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

import numpy as np
import matplotlib.pyplot as plt

"""## Fingerprint length optimization"""

fp_sizes = [128, 512, 1024]

n_estimators = [40, 60, 80, 100]

fp_results = []

n_runs = 30

for fp_s in fp_sizes:

  fingerprints_optim, fingerprint_features = extract_fingerprints(train_data, fp_s)

  acc_nb = 0
  f1_nb = 0
  auc_nb = 0

  acc_lr = 0
  f1_lr = 0
  auc_lr = 0

  for i in range(n_runs): # run n_runs times and average the results
    # create a train test split for current iteration
    X_train, X_validation, y_train, y_validation = train_test_split(fingerprints_optim, train_data['ACTIVE'], test_size=0.3, random_state=i)

    # fit and predict
    nb = make_pipeline(StandardScaler(),GaussianNB())
    nb.fit(X_train, y_train)
    y_valid_nb = nb.predict(X_validation)

    lr = make_pipeline(StandardScaler(),LogisticRegression())
    lr.fit(X_train, y_train)
    y_valid_lr = lr.predict(X_validation)

    # store scores
    acc_nb += accuracy_score(y_validation, y_valid_nb)
    f1_nb += f1_score(y_validation, y_valid_nb)
    auc_nb += roc_auc_score(y_validation, y_valid_nb)

    acc_lr += accuracy_score(y_validation, y_valid_lr)
    f1_lr += f1_score(y_validation, y_valid_lr)
    auc_lr += roc_auc_score(y_validation, y_valid_lr)

  acc_nb /= n_runs
  f1_nb /= n_runs
  auc_nb /= n_runs

  acc_lr /= n_runs
  f1_lr /= n_runs
  auc_lr /= n_runs




  # store results for current fp size in the dataframe
  fp_results.append([fp_s, 'nb', acc_nb, f1_nb, auc_nb])
  fp_results.append([fp_s, 'lr', acc_lr, f1_lr, auc_lr])


  for n in n_estimators:

    acc_rfc = 0
    f1_rfc = 0
    auc_rfc = 0

    for i in range(n_runs): # run 50 times and average the results
      # create a train test split for current iteration
      X_train, X_validation, y_train, y_validation = train_test_split(fingerprints_optim, train_data['ACTIVE'], test_size=0.3, random_state=i)

      rfc = make_pipeline(StandardScaler(), RandomForestClassifier(n_estimators=n))
      rfc.fit(X_train, y_train)
      y_valid_rfc = rfc.predict(X_validation)

      acc_rfc += accuracy_score(y_validation, y_valid_rfc)
      f1_rfc += f1_score(y_validation, y_valid_rfc)
      auc_rfc += roc_auc_score(y_validation, y_valid_rfc)

    acc_rfc /= n_runs
    f1_rfc /= n_runs
    auc_rfc /= n_runs

    fp_results.append([fp_s, f'rfc_{n}', acc_rfc, f1_rfc, auc_rfc])



rez = pd.DataFrame(data=fp_results, columns=['fp_size', 'classifier', 'acc', 'f1', 'auc'])

#rez.to_csv(FINGERPRINT_OPTIM_RES_PATH)

fp_size_rez = pd.read_csv(FINGERPRINT_OPTIM_RES_PATH)

fp_size_rez.sort_values('auc', ascending=False).head(10)

# Train and validation split

seed = 10
X_train, X_validation, y_train, y_validation = train_test_split(train_data.drop(['INDEX', 'SMILES', 'MOL', 'ACTIVE'], axis=1), train_data['ACTIVE'], test_size=0.3, random_state=seed)

"""## Custom feature selection

In order to determine the quality of custom extracted features we perform feature selection

For this we use Random Forest Classifier
"""

X, y = train_data[custom_features_all], train_data['ACTIVE']

rfc = RandomForestClassifier()

rfc.fit(X, y)

importances = rfc.feature_importances_
std = np.std([tree.feature_importances_ for tree in rfc.estimators_], axis=0)

forest_importances = pd.Series(importances, index=custom_features_all)


fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=std, ax=ax)
ax.set_title("Feature importances using MDI")
ax.set_ylabel("Mean decrease in impurity")
fig.tight_layout()

"""### Determining model performance with different subsets of custom featureset

The feature set is defined with regards to the threshold on mean decrease in impurity
"""

def train_validate_model(classifier, classifier_name, X, y,storage, print_rez=True):

  pipe = make_pipeline(StandardScaler(), classifier)

  # perform cross validation
  y_train = []
  y_pred = []

  cv = StratifiedKFold(n_splits=6)
  for fold, (train, valid) in enumerate(cv.split(X, y)):

    pipe.fit(X.loc[train], y.loc[train])

    y_train.extend(y.loc[valid].tolist())
    y_pred.extend(pipe.predict(X.loc[valid]))

  acc= accuracy_score(y_train, y_pred)
  f1= f1_score(y_train, y_pred)

  auc = roc_auc_score(y_train, y_pred)

  if print_rez:
    print(f'{classifier_name} :')
    print(f'Accuracy: {acc}  F1: {f1}')
    print(f'AUC = {auc}')

  
  storage.append([classifier_name, acc, f1, auc])

  return acc, f1, auc

thresholds = [0.01, 0.015, 0.02, 0.05, 0.1, 0.15]

res = [] # features, classifier, acc, f1, auc
n_runs = 1

y = train_data['ACTIVE']
smote = SMOTE(sampling_strategy=0.2, random_state=10)

n = 30


for t in thresholds:
  feats = forest_importances.loc[lambda x : x > t].index.tolist()   
  X = train_data[feats]

  X_smote, y_smote = smote.fit_resample(X, y)
  
  acc_smote = 0
  f1_smote = 0
  auc_smote = 0

  print(t)
  for i in range(n_runs):
    classifier = RandomForestClassifier(n_estimators=n)
    acc, f1, auc = train_validate_model(classifier, f'RFC_{n} - smote', X_smote, y_smote, [], print_rez=False)

    acc_smote += acc
    f1_smote += f1
    auc_smote += auc

  res.append(['RFC-smote', acc_smote/n_runs, f1_smote/n_runs, auc_smote/n_runs, feats])


features_optim = pd.DataFrame(data=res, columns=['classifier', 'acc', 'f1', 'auc', 'features'])

#features_optim.to_csv(FEATURES_OPTIM_RES_PATH)

features_optim = pd.read_csv(FEATURES_OPTIM_RES_PATH)

custom_features = features_optim.loc[2, 'features'][1:-1].replace('\'', '').split(', ')

print(custom_features)

fp_features = [f'fp_{i}' for i in range(512)]

"""# Models

- Logistic regression
- Naive Bayes
- Random Forest
- XGBoost

"""

custom_features_res = []
fingerprint_res = []
combined_res = []

# storing dataset on the disc

#train_data.to_csv(TRAIN_DATASET_PREPARED_PATH)

train_data = pd.read_csv(TRAIN_DATASET_PREPARED_PATH, dtype = {'ACTIVE': int})
train_data.head()

"""### Fingerprint dataset

"""

seed = 10
X = train_data[fp_features]
y = train_data['ACTIVE']

smote = SMOTE(sampling_strategy=0.1, random_state=seed)
X_smote, y_smote = smote.fit_resample(X, y)

ros = RandomOverSampler(sampling_strategy=0.1, random_state=seed)
X_ros, y_ros = ros.fit_resample(X, y)

# Logistic Regression

lr = LogisticRegression()

acc, f1, auc = train_validate_model(lr, 'Logistic Regression', X, y, storage=fingerprint_res)

acc, f1, auc = train_validate_model(lr, 'Logistic Regression smote', X_smote, y_smote, storage=fingerprint_res)

acc, f1, auc = train_validate_model(lr, 'Logistic Regression ros', X_ros, y_ros, storage=fingerprint_res)

# Naive Bayes

nb = GaussianNB()

acc, f1, auc = train_validate_model(nb, 'Naive Bayes Classifier', X, y, storage=fingerprint_res)

acc, f1, auc = train_validate_model(nb, 'Naive Bayes Classifier smote', X_smote, y_smote, storage=fingerprint_res)

acc, f1, auc = train_validate_model(nb, 'Naive Bayes Classifier ros', X_ros, y_ros, storage=fingerprint_res)

# Random Forest Classifier

rfc = RandomForestClassifier(n_estimators=50)

print('Original dataset')
acc, f1, auc = train_validate_model(rfc, 'Random Forest Classifier',X, y, storage=fingerprint_res)

print('SMOTE dataset')
acc, f1, auc = train_validate_model(rfc, 'Random Forest Classifier smote', X_smote, y_smote, storage=fingerprint_res)

print('ROS dataset')
acc, f1, auc = train_validate_model(rfc, 'Random Forest Classifier ros', X_ros, y_ros, storage=fingerprint_res)

# XGBoost

n_estimators =[40, 60, 80, 100]

for n in n_estimators:
  xgb = XGBClassifier(n_estimators=n, scale_pos_weight= (len(y) - sum(y)) / sum(y))

  acc, f1, auc  = train_validate_model(xgb, f'XGB Classifier_{n}', X, y, storage=fingerprint_res)

  acc, f1, auc  = train_validate_model(xgb, f'XGB Classifier_{n}_smote', X_smote, y_smote, storage=fingerprint_res)

  acc, f1, auc = train_validate_model(xgb, f'XGB Classifier_{n}_ros', X_ros, y_ros, storage=fingerprint_res)

"""### Custom features dataset"""

seed = 10
X, y = train_data[custom_features], train_data['ACTIVE']

smote = SMOTE(sampling_strategy=0.1, random_state=seed)
X_smote, y_smote = smote.fit_resample(X, y)

ros = RandomOverSampler(sampling_strategy=0.1, random_state=seed)
X_ros, y_ros = ros.fit_resample(X, y)

# Logistic Regression

lr = LogisticRegression()

acc, f1, auc  = train_validate_model(lr, 'Logistic Regression', X, y, storage=custom_features_res)

acc, f1, auc  = train_validate_model(lr, 'Logistic Regression smote', X_smote, y_smote, storage=custom_features_res)

acc, f1, auc = train_validate_model(lr, 'Logistic Regression ros', X_ros, y_ros, storage=custom_features_res)

# Naive Bayes

nb = GaussianNB()

acc, f1, auc  = train_validate_model(nb, 'Naive Bayes Classifier', X, y, storage=custom_features_res)

acc, f1, auc  = train_validate_model(nb, 'Naive Bayes Classifier smote', X_smote, y_smote, storage=custom_features_res)

acc, f1, auc = train_validate_model(nb, 'Naive Bayes Classifier ros', X_ros, y_ros, storage=custom_features_res)

# Random Forest Classifier

n = 50
rfc = RandomForestClassifier(n_estimators=n)

acc, f1, auc  = train_validate_model(rfc, f'Random Forest Classifier_{n}', X, y, storage=custom_features_res)

acc, f1, auc  = train_validate_model(rfc, f'Random Forest Classifier_{n}_smote', X_smote, y_smote, storage=custom_features_res)

acc, f1, auc  = train_validate_model(rfc, f'Naive Bayes Classifier_{n}_ros', X_ros, y_ros, storage=custom_features_res)

# XGBoost

n_estimators = [40, 60, 80, 100]

for n in n_estimators:
  xgb = XGBClassifier(n_estimators=n, scale_pos_weight= (len(y) - sum(y)) / sum(y))

  acc, f1, acc = train_validate_model(xgb, f'XGB Classifier_{n}', X, y, storage=custom_features_res)

  acc, f1, acc = train_validate_model(xgb, f'XGB Classifier_{n}_smote', X_smote, y_smote, storage=custom_features_res)

  acc, f1, auc = train_validate_model(xgb, f'XGB Classifier_{n}_ros', X_ros, y_ros, storage=custom_features_res)

"""### Combined features dataset"""

seed = 10
X, y = train_data.drop(['INDEX', 'SMILES', 'MOL', 'ACTIVE'] , axis=1), train_data['ACTIVE']

smote = SMOTE(sampling_strategy=0.1, random_state=seed)
X_smote, y_smote = smote.fit_resample(X, y)

ros = RandomOverSampler(sampling_strategy=0.1, random_state=seed)
X_ros, y_ros = ros.fit_resample(X, y)

# Logistic Regression

lr = LogisticRegression()

acc, f1, auc  = train_validate_model(lr, 'Logistic Regression', X, y, storage=combined_res)

acc, f1, auc  = train_validate_model(lr, 'Logistic Regression smote', X_smote, y_smote, storage=combined_res)

acc, f1, auc = train_validate_model(lr, 'Logistic Regression ros', X_ros, y_ros, storage=combined_res)

# Naive Bayes

nb = GaussianNB()

acc, f1, auc  = train_validate_model(nb, 'Naive Bayes Classifier', X, y, storage=combined_res)

acc, f1, auc  = train_validate_model(nb, 'Naive Bayes Classifier smote', X_smote, y_smote, storage=combined_res)

acc, f1, auc = train_validate_model(nb, 'Naive Bayes Classifier ros', X_ros, y_ros, storage=combined_res)

# Random Forest Classifier

n = 50
rfc = RandomForestClassifier(n_estimators=n)

print('Original dataset')
acc, f1, auc = train_validate_model(rfc, f'Random Forest Classifier_{n}', X, y, storage=combined_res)

print('SMOTE dataset')
acc, f1, auc = train_validate_model(rfc, f'Random Forest Classifier_{n}_smote', X_smote, y_smote, storage=combined_res)

print('ROS dataset')
acc, f1, auc = train_validate_model(rfc, f'Random Forest Classifier_{n}_ros', X_ros, y_ros, storage=combined_res)

# XGBoost

n_estimators =[ 40, 60, 80, 100]

for n in n_estimators:
  xgb = XGBClassifier(n_estimators=n, scale_pos_weight= (len(y) - sum(y)) / sum(y))

  y_true, y_pred, acc = train_validate_model(xgb, f'XGB Classifier_{n}', X, y, storage=combined_res)

  y_true, y_pred, acc = train_validate_model(xgb,f'XGB Classifier_{n}_smote', X_smote, y_smote, storage=combined_res)

  y_true, y_pred, auc = train_validate_model(xgb, f'XGB Classifier_{n}_ros', X_ros, y_ros, storage=combined_res)

"""Results"""

fp_res_df = pd.DataFrame(data=fingerprint_res, columns=['classifier', 'acc', 'f1', 'auc'])
#fp_res_df.to_csv(FP_RES_PATH)

custom_res_df = pd.DataFrame(data=custom_features_res, columns=['classifier', 'acc', 'f1', 'auc'])
#custom_res_df.to_csv(CUSTOM_RES_PATH)

combined_res_df = pd.DataFrame(data=combined_res, columns=['classifier', 'acc', 'f1', 'auc'])
#combined_res_df.to_csv(COMBINED_RES_PATH)

"""Extract optimal features for the test dataset

# Test set label prediction
"""

test_data = pd.read_csv(TEST_DATASET_PATH)


test_data['MOL'] = extract_mol(test_data)
test_data['CHARGE'] = extract_charge(test_data)
test_data['NUM_C'] = extract_num_atoms(test_data, 'C')
test_data['NUM_O'] = extract_num_atoms(test_data, 'O')
test_data['NUM_N'] = extract_num_atoms(test_data, 'N')
test_data['NUM_HEAVY'] = extract_heavy_atom_count(test_data)
test_data['MASS'] = calculate_mass(test_data)
test_data['ACCEPTORS'], train_data['DONORS'] = extract_donors_and_acceptors(test_data)
test_data['AL_OOF'] = extract_Al_OOF(test_data)
test_data['AL_OH'] = extract_Al_OH(test_data)
test_data['NUM_NHOH'] = extract_NHOH(test_data)
test_data['NUM_RINGS'] = extract_num_rings(test_data)
test_data['NUM_AROM_RINGS'] = extract_num_aromatic_rings(test_data)
test_data['NUM_SAT_RINGS'] = extract_num_saturated_rings(test_data)
test_data['NUM_HETCYCLES'] = extract_saturated_heterocycles(test_data)
test_data['NUM_CARBCYCLES'] = extract_saturated_carbocycles(test_data)

# newly added 
test_data['ROTATABLE_BONDS'] = calc_rotatable_bonds(test_data)
test_data['LOG_P'] = extract_log_p(test_data)
test_data['FUNC_GROUP'] = extract_functional_groups(test_data)
test_data['MR'] = extract_mol_mr(test_data)


# fingerprints
size_fp = 512
fingerprints, fp_features = extract_fingerprints(test_data, size_fp)
test_data = test_data.join(fingerprints)

#test_data.to_csv(TEST_DATASET_PREPARED_PATH)

test_data = pd.read_csv(TEST_DATASET_PREPARED_PATH, dtype = {'ACTIVE': int})

test_data.head()

"""### Training the model"""

X, y = train_data[fp_features], train_data['ACTIVE']

X_test = test_data[fp_features]

seed=10

smote = SMOTE(sampling_strategy=0.1, random_state=seed)
X_smote, y_smote = smote.fit_resample(X, y)

rez = []
classifiers = [RandomForestClassifier(40), RandomForestClassifier(50), RandomForestClassifier(60), RandomForestClassifier(70)]
for c in classifiers:

  train_validate_model(c, f'RFC', X_smote, y_smote, rez)

classifier = RandomForestClassifier(50)
classifier.fit(X_smote, y_smote)

y_pred_proba = classifier.predict_proba(X_test)

y_pred_proba[:, 1]

with open(RESULT_PATH, "w") as f:
  f.write('0.9373' + '\n')
  for i in y_pred_proba[:, 1]:
    f.write("{:e}\n".format(i))

